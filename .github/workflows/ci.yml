name: CI

on:
  push:
    branches: [ main, master ]
  pull_request:
    branches: [ main, master ]
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  backend:
    name: Backend (Python)
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: .
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt', 'requirements-analytics.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-analytics.txt

      - name: Test Analytics Integration
        run: |
          python -c "
          import sys
          try:
              import supabase, sqlalchemy, pandas, schedule
              print('‚úÖ Analytics dependencies installed successfully')
          except ImportError as e:
              print(f'‚ùå Missing analytics dependency: {e}')
              sys.exit(1)
          "


  frontend:
    name: Frontend (Node)
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: frontend
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Use Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Install dependencies
        run: npm ci

      - name: Build
        env:
          CI: false
        run: npm run build


  deploy:
    name: Deploy to Linode
    runs-on: ubuntu-latest
    needs: [backend, frontend]
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master'
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Prepare artifact (filter out dev caches)
        run: |
          rm -rf .git

      - name: Upload project to server (key or password)
        uses: appleboy/scp-action@v0.1.7
        with:
          host: ${{ secrets.SSH_HOST }}
          username: ${{ secrets.SSH_USER }}
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          password: ${{ secrets.SSH_PASSWORD }}
          source: "."
          target: "${{ secrets.REMOTE_APP_DIR }}"

      - name: Run remote deployment (key or password)
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.SSH_HOST }}
          username: ${{ secrets.SSH_USER }}
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          password: ${{ secrets.SSH_PASSWORD }}
          script_stop: true
          script: |
            set -euo pipefail
            export APP_DIR=${{ secrets.REMOTE_APP_DIR }}
            # Install Docker & Compose if missing
            if ! command -v docker >/dev/null 2>&1; then
              curl -fsSL https://get.docker.com | sh
              systemctl enable docker || true
              systemctl start docker || true
            fi
            if ! docker compose version >/dev/null 2>&1; then
              DOCKER_CONFIG=${HOME}/.docker mkdir -p ${DOCKER_CONFIG}/cli-plugins
              curl -SL https://github.com/docker/compose/releases/download/v2.29.7/docker-compose-linux-$(uname -m) -o ${DOCKER_CONFIG}/cli-plugins/docker-compose
              chmod +x ${DOCKER_CONFIG}/cli-plugins/docker-compose
            fi

            mkdir -p "$APP_DIR"
            cd "$APP_DIR"

            # No local storage directories needed - using Supabase Storage
            
            # Ensure env files exist (backend .env)
            if [ -n "${{ secrets.BACKEND_ENV }}" ]; then
              echo "${{ secrets.BACKEND_ENV }}" > .env
            fi
            # Frontend env
            if [ -n "${{ secrets.FRONTEND_ENV }}" ]; then
              echo "${{ secrets.FRONTEND_ENV }}" > frontend/.env.production
            elif [ ! -f frontend/.env.production ]; then
              echo "REACT_APP_API_URL=/api" > frontend/.env.production
            fi

            # Stop services before rebuild to handle SQLite file locks
            docker compose -f docker-compose.yml down || true
            
            # Build and start services
            docker compose -f docker-compose.yml build --no-cache
            docker compose -f docker-compose.yml up -d
            
            # Clear user database for fresh start (ONE-TIME ONLY)
            echo "üßπ Clearing user database for fresh production start..."
            docker compose exec -T api python -c "
            import os
            import psycopg2
            from urllib.parse import urlparse
            
            # Parse database URL
            db_url = os.getenv('DATABASE_URL')
            if not db_url:
                print('‚ùå DATABASE_URL not found')
                exit(1)
                
            # Connect to database
            result = urlparse(db_url)
            conn = psycopg2.connect(
                database=result.path[1:],
                user=result.username,
                password=result.password,
                host=result.hostname,
                port=result.port
            )
            
            cur = conn.cursor()
            
            # Clear user data (in dependency order)
            tables_to_clear = [
                'job_views',
                'job_favorites', 
                'job_applications',
                'visa_verification_history',
                'visa_verifications',
                'user_sessions',
                'users'
            ]
            
            print('üóëÔ∏è Clearing user data...')
            for table in tables_to_clear:
                try:
                    cur.execute(f'DELETE FROM {table}')
                    count = cur.rowcount
                    print(f'  ‚úÖ Cleared {count} records from {table}')
                except Exception as e:
                    print(f'  ‚ö†Ô∏è Could not clear {table}: {e}')
            
            # Reset auto-increment sequences
            sequences = [
                'users_id_seq',
                'visa_verifications_id_seq', 
                'visa_verification_history_id_seq',
                'user_sessions_id_seq',
                'job_favorites_id_seq',
                'job_applications_id_seq',
                'job_views_id_seq'
            ]
            
            print('üîÑ Resetting sequences...')
            for seq in sequences:
                try:
                    cur.execute(f'ALTER SEQUENCE {seq} RESTART WITH 1')
                    print(f'  ‚úÖ Reset {seq}')
                except Exception as e:
                    print(f'  ‚ö†Ô∏è Could not reset {seq}: {e}')
            
            # Commit changes
            conn.commit()
            cur.close()
            conn.close()
            
            print('‚úÖ User database cleared successfully!')
            print('üìä System data (jobs, companies) preserved')
            "
            
            # Deploy Analytics
            echo "üîß Deploying Analytics System..."
            
            # Install analytics dependencies in the running container
            docker compose exec -T api pip install supabase sqlalchemy pandas schedule requests python-dotenv || true
            
            # Copy analytics files to container
            docker compose exec -T api mkdir -p /app/analytics || true
            docker cp analytics_export.py $(docker compose ps -q api):/app/analytics_export.py || true
            docker cp periodic_analytics.py $(docker compose ps -q api):/app/periodic_analytics.py || true
            docker cp analytics_dashboard.py $(docker compose ps -q api):/app/analytics_dashboard.py || true
            docker cp integrate_analytics.py $(docker compose ps -q api):/app/integrate_analytics.py || true
            
            # Set up analytics cron jobs on the host (every hour)
            echo "‚è∞ Setting up analytics cron jobs (every hour)..."
            (crontab -l 2>/dev/null; echo "# Joborra Analytics Export Jobs - Every Hour") | crontab - || true
            (crontab -l 2>/dev/null; echo "0 * * * * cd $APP_DIR && docker compose exec -T api python /app/analytics_export.py >> /var/log/joborra/analytics.log 2>&1") | crontab - || true
            
            # Create analytics log directory
            mkdir -p /var/log/joborra
            touch /var/log/joborra/analytics.log
            
            # Test analytics deployment
            echo "üß™ Testing analytics deployment..."
            docker compose exec -T api python -c "
            import sys
            try:
                import supabase, sqlalchemy, pandas
                print('‚úÖ Analytics dependencies available in container')
            except ImportError as e:
                print(f'‚ö†Ô∏è  Analytics dependency issue: {e}')
            " || true
            
            echo "‚úÖ Analytics deployment completed"
            
            # Verify deployment
            sleep 10
            docker compose logs api --tail=50
            # Cleanup old images
            docker image prune -f || true
